{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder. This may be different between users\n",
    "location = 'data/secure/'\n",
    "\n",
    "data_path = pathlib.Path(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rename folders\n",
    "We ran into some issues where the extremely deep paths and long filenames were causing errors with our ability to load the data. To correct this issue, we renamed folders by changing the second long folder name to `feature_masks`. For example,\n",
    "\n",
    "`data/SECURE_KEY/2036311/DPm.1.2.840.113681.2863050713.1318230214.3060.1227/DPm.1.2.840.113681.2863050713.1318230214.3060`\n",
    "\n",
    "becomes \n",
    "\n",
    "`data/SECURE_KEY/2036311/DPm.1.2.840.113681.2863050713.1318230214.3060.1227/feature_masks`\n",
    "\n",
    "This does not cause any confusion or ambiguity, as both the filenames themselves, as well as the parent folders contain the same information. The directory structure within the directory given above by `location` is then:\n",
    "\n",
    "```\n",
    "location\n",
    "├── 2036311\n",
    "|   └── DPm.1.2.840.113681.2863050713.1318230214.3060.1227\n",
    "|       └── feature_masks\n",
    "|           └── feature\n",
    "|               └── DPm ... .nii.gz\n",
    "|               └── ...\n",
    "|               └── DPm ... .nii.gz\n",
    "|           └── mask\n",
    "|               └── DPm ... 0_mask_win_97_sliding_97_mean.nii.gz\n",
    "|               └── DPm ... _mask.nii.gz\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename folders\n",
    "for feature_dir in data_path.glob('*/*/*/feature/'):\n",
    "    parent_dir = feature_dir.parent\n",
    "    parent_dir.rename(parent_dir.parent / 'feature_masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train/test splitting\n",
    "We have been given data on roughly 570 patients. There is some ambiguity in the number, though, as we have case/control status on 575 patients, while we have feature maps for 569 patients. Of the 569 patients with features, 114 were cases and the remaining 455 were controls. In the context of breast cancer prediction, this means 114 of the patients eventually developed breast cancer while the others did not.\n",
    "\n",
    "Among the patients with extracted feature maps, 533 patients had two images (corresponding to left and right breast), while the remaining 36 had an image for one side only. In consultation with Dr. Aimilia Gastounioti, we decided the most sensible approach would be to treat each image as a separate sample. Using this approach, we have 1102 total samples.\n",
    "\n",
    "We opted for an 80/20 train/test split, a standard split fraction. This means that 455 patients are assigned to the traing set and 114 are assigned to the test set. We took care to ensure that the case/control ratio within both groups reflected the overall distributions. This can be seen below, where 20.2% of the patients assigned to the training set were cases, and 19.3% of the test set were cases. These numbers are not exactly 20%, though they make sense in light of the fact that the number of patients is not evenly divisible in the fraction we desire.\n",
    "\n",
    "To eliminate a source of bias in our model, we did not allow patients with multiple images to have their data split between training and testing data. This means that patients with two images always had both images together, and we split the data by patients rather than by sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "# Get list of patients with feature maps\n",
    "patients_list = [subdir.name for subdir in data_path.glob('*/') if subdir.is_dir()]\n",
    "\n",
    "# Read in case/control information\n",
    "case_control_df = pd.read_excel('controlcase.xlsx')\n",
    "\n",
    "# Create a dictionary mapping patient_id to case/control status\n",
    "patient_id_to_case = case_control_df[['DummyID', 'Class']].set_index('DummyID')['Class'].to_dict()\n",
    "\n",
    "# Set random seed so that split can be done reproducibly\n",
    "np.random.seed(0)\n",
    "\n",
    "# Pick patients whose images will be in train/test sets\n",
    "training_patients = np.random.choice(patients_list, replace=False, size=455)\n",
    "testing_patients = [patient for patient in patients_list if patient not in training_patients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training patients: 455\n",
      "Testing patients: 114\n",
      "\n",
      "Percent cases in training data: 0.2021978021978022\n",
      "Percent cases in testing data: 0.19298245614035087\n"
     ]
    }
   ],
   "source": [
    "# Verify the train/test split sizes\n",
    "print(f'Training patients: {len(training_patients)}\\n'\n",
    "      f'Testing patients: {len(testing_patients)}\\n')\n",
    "\n",
    "# Verify the relative numbers of cases and controls between training and testing\n",
    "num_training_cases = sum([patient_id_to_case[int(patient_id)] for patient_id in training_patients])\n",
    "num_testing_cases = sum([patient_id_to_case[int(patient_id)] for patient_id in testing_patients])\n",
    "\n",
    "print(f'Percent cases in training data: {num_training_cases / len(training_patients)}\\n'\n",
    "      f'Percent cases in testing data: {num_testing_cases / len(testing_patients)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the ids of patients in training/testing to a file\n",
    "# so that our methods can be replicated exactly\n",
    "with open('data/training_patients.txt', 'w') as train_file:\n",
    "    train_file.write('patient_id,case_status\\n')\n",
    "    for patient_id in training_patients:\n",
    "        case_status = patient_id_to_case[int(patient_id)]\n",
    "        train_file.write(f'{patient_id},{case_status}\\n')\n",
    "        \n",
    "with open('data/testing_patients.txt', 'w') as test_file:\n",
    "    test_file.write('patient_id,case_status\\n')\n",
    "    for patient_id in testing_patients:\n",
    "        case_status = patient_id_to_case[int(patient_id)]\n",
    "        test_file.write(f'{patient_id},{case_status}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load and process feature maps\n",
    "Below, we extract all feature maps, apply the mask, sort, and combine features into 4D arrays. Then, we normalize features first across samples then within samples, just as was performed in the code provided for us.\n",
    "\n",
    "Throughout the process, we are very careful to ensure that features are always in correspondence with their patient_id or case/control status. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Load data into lists of feature dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CNN\n",
    "feature_masks = data_path.glob('*/*/feature_masks/')\n",
    "\n",
    "train_features = list()\n",
    "train_classes = list()\n",
    "test_features = list()\n",
    "test_classes = list()\n",
    "\n",
    "for feature_mask_path in feature_masks:\n",
    "    # Get patient's dummy id\n",
    "    patient_id = feature_mask_path.parent.parent.name\n",
    "\n",
    "    # Get patient's case/control status\n",
    "    patient_class = patient_id_to_case[int(patient_id)]\n",
    "\n",
    "    # Load the sample's mask\n",
    "    mask_path = list((feature_mask_path / 'mask').glob('*_mean.nii.gz'))[0].as_posix()\n",
    "    mask = nib.load(mask_path).get_data().T\n",
    "\n",
    "    # Iterate through all feature maps. Load and apply mask to each.\n",
    "    patient_features = dict()\n",
    "    features_paths = (feature_mask_path / 'feature').glob('*.nii.gz')\n",
    "    for feature_path in features_paths:\n",
    "\n",
    "        # Load feature map and apply mask\n",
    "        feature_map = np.nan_to_num(nib.load(feature_path.as_posix()).get_data().T)\n",
    "        masked_feature_map = np.multiply(feature_map, mask)\n",
    "\n",
    "        # Extract the feature name from its filename. Eg: norm_win_97_sliding_97_box_counting from\n",
    "        # DPm.1.2.840.113681.2863050709.1375427076.3328_norm_win_97_sliding_97_box_counting.nii.gz\n",
    "        feature_name = re.search('(?<=_).+(?=\\.nii\\.gz)', feature_path.name).group()  # noqa: W605\n",
    "        patient_features[feature_name] = masked_feature_map\n",
    "\n",
    "    # Get patient's train/test category and add the data in the corresponding lists\n",
    "    is_test = patient_id in testing_patients\n",
    "    is_train = patient_id in training_patients\n",
    "    assert not (is_test and is_train)\n",
    "    if is_test:\n",
    "        test_features.append(patient_features)\n",
    "        test_classes.append(patient_class)\n",
    "    elif is_train:\n",
    "        train_features.append(patient_features)\n",
    "        train_classes.append(patient_class)\n",
    "    else:\n",
    "        raise ValueError('Patient ID not found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Combine the data into 4D arrays\n",
    "Very importantly, ensure that the features are always ordered the same way for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data in 4D arrays\n",
    "\n",
    "# Create an ordered list of feature names to ensure they are in the same\n",
    "# order for every sample in the training and testing data\n",
    "ordered_feature_names = sorted(train_features[0].keys())\n",
    "\n",
    "# Save the data in 4D arrays\n",
    "train_data = np.zeros((len(train_features), 34, 26, 29))\n",
    "test_data = np.zeros((len(test_features), 34, 26, 29))\n",
    "\n",
    "for sample_number, sample_dict in enumerate(train_features):\n",
    "    for feature_number, feature_name in enumerate(ordered_feature_names):\n",
    "        # Crop images to all be 34 x 26. Some are originally larger at 42 x 37\n",
    "        train_data[sample_number, :, :, feature_number] = sample_dict[feature_name][0:34, 0:26]\n",
    "\n",
    "for sample_number, sample_dict in enumerate(test_features):\n",
    "    for feature_number, feature_name in enumerate(ordered_feature_names):\n",
    "        # Crop images to all be 34 x 26. Some are originally larger at 42 x 37\n",
    "        test_data[sample_number, :, :, feature_number] = sample_dict[feature_name][0:34, 0:26]\n",
    "\n",
    "# Convert label lists to numpy arrays\n",
    "train_classes = np.asarray(train_classes)\n",
    "test_classes = np.asarray(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Normalize the feature maps\n",
    "As was done in the preprocessing code from the 2016 paper, we first normalize across samples, then normalize features within samples. Note that we add a term, `epsilon` to the divisors below. This is because some features are zero across all samples or across all feature_maps within sample. In these cases, we would be dividing by zero, which would introduce unwanted `nan` terms into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "\n",
    "# Normalize the data across samples\n",
    "# Combine the data and find the largest magnitude values for each feature\n",
    "full_data = np.concatenate((train_data, test_data))\n",
    "max_image = np.abs(full_data).max(axis=0)\n",
    "\n",
    "train_data = np.divide(train_data, max_image + epsilon)\n",
    "test_data = np.divide(test_data, max_image + epsilon)\n",
    "\n",
    "# Normalize feature maps so that the maximum value in each is 1.\n",
    "# # This is the within-sample normalization that was performed\n",
    "# # in the preprocessing code we received from the 2016 paper\n",
    "for data_source in (train_data, test_data):\n",
    "    for sample_number, sample in enumerate(data_source):\n",
    "        for feature_number in range(29):\n",
    "            feature_map = sample[:, :, feature_number]\n",
    "            max_val = np.abs(feature_map).max()\n",
    "            data_source[sample_number, :, :, feature_number] = np.divide(feature_map, max_val + epsilon)\n",
    "\n",
    "# Save the data as pickled tuples of data, labels\n",
    "training_set = (train_data, train_classes)\n",
    "testing_set = (test_data, test_classes)\n",
    "\n",
    "train_data_path = data_path.parent.joinpath('train_data.pkl')\n",
    "test_data_path = data_path.parent.joinpath('test_data.pkl')\n",
    "\n",
    "with open(train_data_path, 'wb') as f:\n",
    "    pickle.dump(training_set, f)\n",
    "\n",
    "with open(test_data_path, 'wb') as f:\n",
    "    pickle.dump(testing_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create and train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D, Activation, Flatten, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set numpy and TensorFlow random seeds in the hopes of making\n",
    "# results reproducible. This will not be possible when using a GPU,\n",
    "# as there may be asynchronous processing for which no random seed\n",
    "# could account.\n",
    "set_random_seed(2)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = to_categorical(train_classes)\n",
    "test_classes = to_categorical(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/cis537/lib/python3.6/site-packages/ipykernel/__main__.py:31: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/michael/miniconda3/envs/cis537/lib/python3.6/site-packages/ipykernel/__main__.py:31: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=880, epochs=100, class_weight={0: 1, 1: ..., validation_data=<keras_pre..., validation_steps=222)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1296 - binary_accuracy: 0.5216 - val_loss: 0.7850 - val_binary_accuracy: 0.1865\n",
      "Epoch 2/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1184 - binary_accuracy: 0.4017 - val_loss: 0.6180 - val_binary_accuracy: 0.8126\n",
      "Epoch 3/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1155 - binary_accuracy: 0.5284 - val_loss: 0.7128 - val_binary_accuracy: 0.3675\n",
      "Epoch 4/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1128 - binary_accuracy: 0.4199 - val_loss: 0.7075 - val_binary_accuracy: 0.4034\n",
      "Epoch 5/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1157 - binary_accuracy: 0.4937 - val_loss: 0.6829 - val_binary_accuracy: 0.5520\n",
      "Epoch 6/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1103 - binary_accuracy: 0.5619 - val_loss: 0.7255 - val_binary_accuracy: 0.4391\n",
      "Epoch 7/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.1123 - binary_accuracy: 0.5233 - val_loss: 0.6388 - val_binary_accuracy: 0.6326\n",
      "Epoch 8/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0949 - binary_accuracy: 0.5324 - val_loss: 0.6506 - val_binary_accuracy: 0.6039\n",
      "Epoch 9/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0593 - binary_accuracy: 0.6023 - val_loss: 0.6734 - val_binary_accuracy: 0.5618\n",
      "Epoch 10/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0459 - binary_accuracy: 0.6188 - val_loss: 0.6561 - val_binary_accuracy: 0.6308\n",
      "Epoch 11/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0392 - binary_accuracy: 0.6273 - val_loss: 0.7597 - val_binary_accuracy: 0.4372\n",
      "Epoch 12/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0263 - binary_accuracy: 0.6278 - val_loss: 0.5885 - val_binary_accuracy: 0.7082\n",
      "Epoch 13/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0013 - binary_accuracy: 0.6284 - val_loss: 0.5892 - val_binary_accuracy: 0.7344\n",
      "Epoch 14/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 1.0130 - binary_accuracy: 0.6699 - val_loss: 0.7553 - val_binary_accuracy: 0.4751\n",
      "Epoch 15/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9727 - binary_accuracy: 0.6722 - val_loss: 0.6426 - val_binary_accuracy: 0.6234\n",
      "Epoch 16/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9421 - binary_accuracy: 0.7028 - val_loss: 0.9528 - val_binary_accuracy: 0.3537\n",
      "Epoch 17/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9734 - binary_accuracy: 0.6545 - val_loss: 0.8717 - val_binary_accuracy: 0.3262\n",
      "Epoch 18/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9434 - binary_accuracy: 0.6670 - val_loss: 0.5599 - val_binary_accuracy: 0.7450\n",
      "Epoch 19/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9583 - binary_accuracy: 0.6364 - val_loss: 0.5700 - val_binary_accuracy: 0.7165\n",
      "Epoch 20/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8984 - binary_accuracy: 0.7250 - val_loss: 0.6425 - val_binary_accuracy: 0.6423\n",
      "Epoch 21/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8734 - binary_accuracy: 0.7085 - val_loss: 0.9001 - val_binary_accuracy: 0.4216\n",
      "Epoch 22/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9546 - binary_accuracy: 0.6284 - val_loss: 0.5924 - val_binary_accuracy: 0.6552\n",
      "Epoch 23/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8485 - binary_accuracy: 0.7148 - val_loss: 0.9209 - val_binary_accuracy: 0.4705\n",
      "Epoch 24/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8984 - binary_accuracy: 0.6960 - val_loss: 0.7832 - val_binary_accuracy: 0.5526\n",
      "Epoch 25/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8380 - binary_accuracy: 0.7381 - val_loss: 0.5678 - val_binary_accuracy: 0.7482\n",
      "Epoch 26/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8863 - binary_accuracy: 0.7000 - val_loss: 0.6302 - val_binary_accuracy: 0.6442\n",
      "Epoch 27/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8943 - binary_accuracy: 0.6926 - val_loss: 0.7799 - val_binary_accuracy: 0.5304\n",
      "Epoch 28/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8424 - binary_accuracy: 0.6926 - val_loss: 0.7271 - val_binary_accuracy: 0.5878\n",
      "Epoch 29/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9297 - binary_accuracy: 0.6835 - val_loss: 0.8708 - val_binary_accuracy: 0.4548\n",
      "Epoch 30/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.8874 - binary_accuracy: 0.6989 - val_loss: 0.8120 - val_binary_accuracy: 0.5357\n",
      "Epoch 31/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7925 - binary_accuracy: 0.7108 - val_loss: 0.6295 - val_binary_accuracy: 0.6432\n",
      "Epoch 32/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.8666 - binary_accuracy: 0.6943 - val_loss: 0.6096 - val_binary_accuracy: 0.6934\n",
      "Epoch 33/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8238 - binary_accuracy: 0.7330 - val_loss: 0.7052 - val_binary_accuracy: 0.6212\n",
      "Epoch 34/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7961 - binary_accuracy: 0.7534 - val_loss: 0.7801 - val_binary_accuracy: 0.5450\n",
      "Epoch 35/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.8462 - binary_accuracy: 0.7455 - val_loss: 0.6859 - val_binary_accuracy: 0.6169\n",
      "Epoch 36/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9092 - binary_accuracy: 0.6960 - val_loss: 0.7164 - val_binary_accuracy: 0.5683\n",
      "Epoch 37/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8101 - binary_accuracy: 0.7136 - val_loss: 0.6822 - val_binary_accuracy: 0.6570\n",
      "Epoch 38/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8153 - binary_accuracy: 0.7125 - val_loss: 0.7078 - val_binary_accuracy: 0.6213\n",
      "Epoch 39/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.9026 - binary_accuracy: 0.6773 - val_loss: 0.6590 - val_binary_accuracy: 0.5811\n",
      "Epoch 40/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8251 - binary_accuracy: 0.7091 - val_loss: 0.7391 - val_binary_accuracy: 0.5411\n",
      "Epoch 41/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8196 - binary_accuracy: 0.7034 - val_loss: 0.7721 - val_binary_accuracy: 0.5820\n",
      "Epoch 42/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7707 - binary_accuracy: 0.7477 - val_loss: 1.0280 - val_binary_accuracy: 0.4722\n",
      "Epoch 43/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8138 - binary_accuracy: 0.7307 - val_loss: 0.8390 - val_binary_accuracy: 0.5582\n",
      "Epoch 44/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8148 - binary_accuracy: 0.7420 - val_loss: 0.9121 - val_binary_accuracy: 0.5234\n",
      "Epoch 45/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8763 - binary_accuracy: 0.6920 - val_loss: 0.7896 - val_binary_accuracy: 0.4489\n",
      "Epoch 46/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8794 - binary_accuracy: 0.6636 - val_loss: 0.6963 - val_binary_accuracy: 0.6091\n",
      "Epoch 47/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7539 - binary_accuracy: 0.7477 - val_loss: 0.9027 - val_binary_accuracy: 0.5097\n",
      "Epoch 48/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7419 - binary_accuracy: 0.7489 - val_loss: 0.9207 - val_binary_accuracy: 0.5094\n",
      "Epoch 49/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8437 - binary_accuracy: 0.7000 - val_loss: 0.6391 - val_binary_accuracy: 0.6974\n",
      "Epoch 50/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7901 - binary_accuracy: 0.7568 - val_loss: 0.6373 - val_binary_accuracy: 0.7214\n",
      "Epoch 51/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7437 - binary_accuracy: 0.7659 - val_loss: 0.7970 - val_binary_accuracy: 0.5810\n",
      "Epoch 52/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7430 - binary_accuracy: 0.7477 - val_loss: 0.7292 - val_binary_accuracy: 0.6126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7314 - binary_accuracy: 0.7614 - val_loss: 0.7170 - val_binary_accuracy: 0.5146\n",
      "Epoch 54/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7662 - binary_accuracy: 0.7068 - val_loss: 0.7795 - val_binary_accuracy: 0.6132\n",
      "Epoch 55/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6973 - binary_accuracy: 0.7773 - val_loss: 0.7013 - val_binary_accuracy: 0.6345\n",
      "Epoch 56/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6826 - binary_accuracy: 0.7773 - val_loss: 0.6385 - val_binary_accuracy: 0.7213\n",
      "Epoch 57/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7591 - binary_accuracy: 0.7545 - val_loss: 0.7107 - val_binary_accuracy: 0.6673\n",
      "Epoch 58/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8658 - binary_accuracy: 0.7080 - val_loss: 0.8596 - val_binary_accuracy: 0.5263\n",
      "Epoch 59/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7668 - binary_accuracy: 0.7500 - val_loss: 0.8003 - val_binary_accuracy: 0.5885\n",
      "Epoch 60/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7084 - binary_accuracy: 0.7659 - val_loss: 0.7491 - val_binary_accuracy: 0.5903\n",
      "Epoch 61/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7874 - binary_accuracy: 0.7250 - val_loss: 0.6799 - val_binary_accuracy: 0.6808\n",
      "Epoch 62/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7183 - binary_accuracy: 0.7705 - val_loss: 0.7950 - val_binary_accuracy: 0.6116\n",
      "Epoch 63/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6917 - binary_accuracy: 0.7898 - val_loss: 0.8595 - val_binary_accuracy: 0.4504\n",
      "Epoch 64/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7576 - binary_accuracy: 0.7386 - val_loss: 0.7372 - val_binary_accuracy: 0.6352\n",
      "Epoch 65/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8171 - binary_accuracy: 0.7250 - val_loss: 0.7644 - val_binary_accuracy: 0.6257\n",
      "Epoch 66/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7413 - binary_accuracy: 0.7375 - val_loss: 0.7192 - val_binary_accuracy: 0.7247\n",
      "Epoch 67/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7757 - binary_accuracy: 0.7307 - val_loss: 0.7110 - val_binary_accuracy: 0.6213\n",
      "Epoch 68/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7754 - binary_accuracy: 0.7295 - val_loss: 0.7842 - val_binary_accuracy: 0.5129\n",
      "Epoch 69/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8290 - binary_accuracy: 0.7409 - val_loss: 0.6475 - val_binary_accuracy: 0.6764\n",
      "Epoch 70/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8703 - binary_accuracy: 0.7182 - val_loss: 0.7459 - val_binary_accuracy: 0.6037\n",
      "Epoch 71/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7692 - binary_accuracy: 0.7125 - val_loss: 0.8656 - val_binary_accuracy: 0.5548\n",
      "Epoch 72/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7341 - binary_accuracy: 0.7784 - val_loss: 0.7045 - val_binary_accuracy: 0.6443\n",
      "Epoch 73/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7759 - binary_accuracy: 0.7273 - val_loss: 0.7590 - val_binary_accuracy: 0.6121\n",
      "Epoch 74/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7661 - binary_accuracy: 0.7773 - val_loss: 0.6612 - val_binary_accuracy: 0.7070\n",
      "Epoch 75/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8152 - binary_accuracy: 0.7500 - val_loss: 0.6009 - val_binary_accuracy: 0.6851\n",
      "Epoch 76/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8001 - binary_accuracy: 0.7602 - val_loss: 0.7668 - val_binary_accuracy: 0.6297\n",
      "Epoch 77/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7736 - binary_accuracy: 0.7057 - val_loss: 0.6947 - val_binary_accuracy: 0.6389\n",
      "Epoch 78/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8123 - binary_accuracy: 0.6989 - val_loss: 0.7556 - val_binary_accuracy: 0.5802\n",
      "Epoch 79/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8335 - binary_accuracy: 0.7136 - val_loss: 0.8169 - val_binary_accuracy: 0.5501\n",
      "Epoch 80/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7977 - binary_accuracy: 0.7534 - val_loss: 0.7386 - val_binary_accuracy: 0.6530\n",
      "Epoch 81/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8002 - binary_accuracy: 0.7386 - val_loss: 0.7356 - val_binary_accuracy: 0.5987\n",
      "Epoch 82/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7597 - binary_accuracy: 0.7557 - val_loss: 0.8347 - val_binary_accuracy: 0.5406\n",
      "Epoch 83/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7243 - binary_accuracy: 0.7557 - val_loss: 0.7978 - val_binary_accuracy: 0.5899\n",
      "Epoch 84/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7931 - binary_accuracy: 0.7216 - val_loss: 0.7117 - val_binary_accuracy: 0.6142\n",
      "Epoch 85/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8317 - binary_accuracy: 0.6511 - val_loss: 0.7208 - val_binary_accuracy: 0.6129\n",
      "Epoch 86/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7511 - binary_accuracy: 0.7625 - val_loss: 0.7903 - val_binary_accuracy: 0.5805\n",
      "Epoch 87/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7663 - binary_accuracy: 0.7682 - val_loss: 0.8712 - val_binary_accuracy: 0.5226\n",
      "Epoch 88/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7435 - binary_accuracy: 0.7489 - val_loss: 0.7764 - val_binary_accuracy: 0.6622\n",
      "Epoch 89/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7927 - binary_accuracy: 0.7261 - val_loss: 0.6001 - val_binary_accuracy: 0.6977\n",
      "Epoch 90/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8406 - binary_accuracy: 0.6955 - val_loss: 0.6656 - val_binary_accuracy: 0.6707\n",
      "Epoch 91/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.7068 - binary_accuracy: 0.7739 - val_loss: 0.7481 - val_binary_accuracy: 0.5903\n",
      "Epoch 92/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.7421 - binary_accuracy: 0.7920 - val_loss: 0.6825 - val_binary_accuracy: 0.6535\n",
      "Epoch 93/100\n",
      "880/880 [==============================] - 2s 3ms/step - loss: 0.7429 - binary_accuracy: 0.7409 - val_loss: 0.6744 - val_binary_accuracy: 0.6564\n",
      "Epoch 94/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6788 - binary_accuracy: 0.7977 - val_loss: 0.8025 - val_binary_accuracy: 0.6259\n",
      "Epoch 95/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6173 - binary_accuracy: 0.7636 - val_loss: 0.6959 - val_binary_accuracy: 0.6440\n",
      "Epoch 96/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.6956 - binary_accuracy: 0.7489 - val_loss: 0.7944 - val_binary_accuracy: 0.6679\n",
      "Epoch 97/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.8020 - binary_accuracy: 0.7148 - val_loss: 0.7147 - val_binary_accuracy: 0.6246\n",
      "Epoch 98/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7680 - binary_accuracy: 0.7341 - val_loss: 0.7178 - val_binary_accuracy: 0.6666\n",
      "Epoch 99/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7781 - binary_accuracy: 0.7170 - val_loss: 0.9004 - val_binary_accuracy: 0.5548\n",
      "Epoch 100/100\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 0.7559 - binary_accuracy: 0.7045 - val_loss: 0.7159 - val_binary_accuracy: 0.6395\n",
      "222/222 [==============================] - 0s 121us/step\n",
      "Weighted test accuracy:  0.6396396380287033\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 30, 22, 10)        7260      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 11, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 9, 10)         1210      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1205      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 9,687\n",
      "Trainable params: 9,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "AUROC: 0.5384589947089946\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator()\n",
    "datagen.fit(train_data)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_datagen.fit(test_data)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), activation='tanh',\n",
    "           data_format='channels_last', input_shape=(34, 26, 29)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(10, kernel_size=(4, 3), activation='tanh'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(5, activation='tanh'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "# callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "#                          verbose=0, mode='auto', baseline=0.7)\n",
    "\n",
    "class_weights = {0: 1, 1: 4}\n",
    "model.fit_generator(datagen.flow(train_data, train_classes, batch_size=1, shuffle=True),\n",
    "#                     callbacks=[callback],\n",
    "                    steps_per_epoch=len(train_data), epochs=100,\n",
    "                    class_weight=class_weights,\n",
    "                    validation_data=val_datagen.flow(test_data, test_classes),\n",
    "                    nb_val_samples=test_data.shape[0])\n",
    "\n",
    "score = model.evaluate(test_data, test_classes)\n",
    "\n",
    "print(\"Weighted test accuracy: \", score[1])\n",
    "preds = model.predict(test_data)\n",
    "auc = roc_auc_score(test_classes, preds)\n",
    "print(model.summary())\n",
    "print(f\"AUROC: {auc}\")\n",
    "\n",
    "model.save('model/most_recent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cis537]",
   "language": "python",
   "name": "conda-env-cis537-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
